{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vgg19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsM02dJFWwd2uGkRaWdE7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesliao2016/HCDR2018/blob/master/Vgg16_borrow_1022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw2WjRAbYfm9",
        "outputId": "ab3f059c-bc10-409b-8229-d992f8aec11b"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def normalization(train_images, test_images):\n",
        "    mean = np.mean(train_images, axis=(0, 1, 2, 3))\n",
        "    std = np.std(train_images, axis=(0, 1, 2, 3))\n",
        "    train_images = (train_images - mean) / (std + 1e-7)\n",
        "    test_images = (test_images - mean) / (std + 1e-7)\n",
        "    return train_images, test_images\n",
        "\n",
        "\n",
        "def load_images():\n",
        "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "    train_images = train_images.astype(np.float32)\n",
        "    test_images = test_images.astype(np.float32)\n",
        "\n",
        "    (train_images, test_images) = normalization(train_images, test_images)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 10)\n",
        "    test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "    # train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
        "    #     buffer_size=10000).batch(batch_size)\n",
        "    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "class ConvBNRelu(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel_size=3, strides=1, padding='SAME', weight_decay=0.0005, rate=0.4, drop=True):\n",
        "        super(ConvBNRelu, self).__init__()\n",
        "        self.drop = drop\n",
        "        self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
        "                                        padding=padding, kernel_regularizer=tf.keras.regularizers.l2(weight_decay))\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "        self.dropOut = keras.layers.Dropout(rate=rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        layer = self.conv(inputs)\n",
        "        layer = tf.nn.relu(layer)\n",
        "        layer = self.batchnorm(layer)\n",
        "        if self.drop:\n",
        "            layer = self.dropOut(layer)\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "class VGG16Model(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(VGG16Model, self).__init__()\n",
        "        self.conv1 = ConvBNRelu(filters=64, kernel_size=[3, 3], rate=0.3)\n",
        "        self.conv2 = ConvBNRelu(filters=64, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling1 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv3 = ConvBNRelu(filters=128, kernel_size=[3, 3])\n",
        "        self.conv4 = ConvBNRelu(filters=128, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling2 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv5 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
        "        self.conv6 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
        "        self.conv7 = ConvBNRelu(filters=256, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling3 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv11 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv12 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv13 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling5 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv14 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv15 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv16 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling6 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.flat = keras.layers.Flatten()\n",
        "        self.dropOut = keras.layers.Dropout(rate=0.5)\n",
        "        self.dense1 = keras.layers.Dense(units=512,\n",
        "                                         activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = keras.layers.Dense(units=10)\n",
        "        self.softmax = keras.layers.Activation('softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        net = self.conv1(inputs)\n",
        "        net = self.conv2(net)\n",
        "        net = self.maxPooling1(net)\n",
        "        net = self.conv3(net)\n",
        "        net = self.conv4(net)\n",
        "        net = self.maxPooling2(net)\n",
        "        net = self.conv5(net)\n",
        "        net = self.conv6(net)\n",
        "        net = self.conv7(net)\n",
        "        net = self.maxPooling3(net)\n",
        "        net = self.conv11(net)\n",
        "        net = self.conv12(net)\n",
        "        net = self.conv13(net)\n",
        "        net = self.maxPooling5(net)\n",
        "        net = self.conv14(net)\n",
        "        net = self.conv15(net)\n",
        "        net = self.conv16(net)\n",
        "        net = self.maxPooling6(net)\n",
        "        net = self.dropOut(net)\n",
        "        net = self.flat(net)\n",
        "        net = self.dense1(net)\n",
        "        net = self.batchnorm(net)\n",
        "        net = self.dropOut(net)\n",
        "        net = self.dense2(net)\n",
        "        net = self.softmax(net)\n",
        "        return net\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(tf.__version__)\n",
        "    print(keras.__version__)\n",
        "\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    # if gpus:\n",
        "    #     # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
        "    #     try:\n",
        "    #         print('start with GPU 7')\n",
        "    #         tf.config.experimental.set_visible_devices(gpus[7], 'GPU')\n",
        "    #         tf.config.experimental.set_memory_growth(gpus[7], True)\n",
        "    #     except RuntimeError as e:\n",
        "    #         # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
        "    #         print(e)\n",
        "\n",
        "    training_epochs = 250\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.1\n",
        "    momentum = 0.9\n",
        "    lr_decay = 1e-6\n",
        "    lr_drop = 20\n",
        "\n",
        "    tf.random.set_seed(777)\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "\n",
        "    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_images()\n",
        "\n",
        "    # data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(train_images)\n",
        "\n",
        "    model = VGG16Model()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
        "                                        decay=1e-6, momentum=momentum, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    model.fit_generator(datagen.flow(train_images, train_labels,\n",
        "                                     batch_size=batch_size), epochs=training_epochs, verbose=2, callbacks=[reduce_lr],\n",
        "                        steps_per_epoch=train_images.shape[0] // batch_size, validation_data=(test_images, test_labels))\n",
        "\n",
        "    model.save_weights('cifar10vgg_custom.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "2.6.0\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "390/390 - 72s - loss: 20.7953 - accuracy: 0.1883 - val_loss: 15.5175 - val_accuracy: 0.1085\n",
            "Epoch 2/250\n",
            "390/390 - 63s - loss: 10.6856 - accuracy: 0.2992 - val_loss: 8.6514 - val_accuracy: 0.1218\n",
            "Epoch 3/250\n",
            "390/390 - 62s - loss: 5.7661 - accuracy: 0.3763 - val_loss: 7.9111 - val_accuracy: 0.1150\n",
            "Epoch 4/250\n",
            "390/390 - 62s - loss: 3.4779 - accuracy: 0.4645 - val_loss: 3.2214 - val_accuracy: 0.3664\n",
            "Epoch 5/250\n",
            "390/390 - 63s - loss: 2.4052 - accuracy: 0.5415 - val_loss: 2.2429 - val_accuracy: 0.5328\n",
            "Epoch 6/250\n",
            "390/390 - 62s - loss: 1.8943 - accuracy: 0.5931 - val_loss: 2.0932 - val_accuracy: 0.5230\n",
            "Epoch 7/250\n",
            "390/390 - 62s - loss: 1.6540 - accuracy: 0.6301 - val_loss: 1.6858 - val_accuracy: 0.5949\n",
            "Epoch 8/250\n",
            "390/390 - 63s - loss: 1.5743 - accuracy: 0.6520 - val_loss: 1.4930 - val_accuracy: 0.6761\n",
            "Epoch 9/250\n",
            "390/390 - 63s - loss: 1.5122 - accuracy: 0.6779 - val_loss: 1.4823 - val_accuracy: 0.6868\n",
            "Epoch 10/250\n",
            "390/390 - 63s - loss: 1.4757 - accuracy: 0.6921 - val_loss: 1.4459 - val_accuracy: 0.7091\n",
            "Epoch 11/250\n",
            "390/390 - 62s - loss: 1.4619 - accuracy: 0.7027 - val_loss: 1.3702 - val_accuracy: 0.7355\n",
            "Epoch 12/250\n",
            "390/390 - 63s - loss: 1.4725 - accuracy: 0.7096 - val_loss: 1.4087 - val_accuracy: 0.7285\n",
            "Epoch 13/250\n",
            "390/390 - 63s - loss: 1.4637 - accuracy: 0.7182 - val_loss: 1.3990 - val_accuracy: 0.7447\n",
            "Epoch 14/250\n",
            "390/390 - 63s - loss: 1.4693 - accuracy: 0.7220 - val_loss: 1.3263 - val_accuracy: 0.7682\n",
            "Epoch 15/250\n",
            "390/390 - 62s - loss: 1.4716 - accuracy: 0.7264 - val_loss: 1.3722 - val_accuracy: 0.7611\n",
            "Epoch 16/250\n",
            "390/390 - 63s - loss: 1.4686 - accuracy: 0.7295 - val_loss: 1.3681 - val_accuracy: 0.7677\n",
            "Epoch 17/250\n",
            "390/390 - 62s - loss: 1.4841 - accuracy: 0.7315 - val_loss: 1.4172 - val_accuracy: 0.7553\n",
            "Epoch 18/250\n",
            "390/390 - 62s - loss: 1.4910 - accuracy: 0.7357 - val_loss: 1.4254 - val_accuracy: 0.7593\n",
            "Epoch 19/250\n",
            "390/390 - 63s - loss: 1.4808 - accuracy: 0.7388 - val_loss: 1.3741 - val_accuracy: 0.7711\n",
            "Epoch 20/250\n",
            "390/390 - 63s - loss: 1.4976 - accuracy: 0.7376 - val_loss: 1.3816 - val_accuracy: 0.7705\n",
            "Epoch 21/250\n",
            "390/390 - 62s - loss: 1.3166 - accuracy: 0.7796 - val_loss: 1.1919 - val_accuracy: 0.8044\n",
            "Epoch 22/250\n",
            "390/390 - 62s - loss: 1.2213 - accuracy: 0.7891 - val_loss: 1.1094 - val_accuracy: 0.8183\n",
            "Epoch 23/250\n",
            "390/390 - 63s - loss: 1.1977 - accuracy: 0.7875 - val_loss: 1.1527 - val_accuracy: 0.8020\n",
            "Epoch 24/250\n",
            "390/390 - 62s - loss: 1.1905 - accuracy: 0.7884 - val_loss: 1.3047 - val_accuracy: 0.7620\n",
            "Epoch 25/250\n",
            "390/390 - 63s - loss: 1.1871 - accuracy: 0.7915 - val_loss: 1.1023 - val_accuracy: 0.8165\n",
            "Epoch 26/250\n",
            "390/390 - 62s - loss: 1.1864 - accuracy: 0.7904 - val_loss: 1.1288 - val_accuracy: 0.8147\n",
            "Epoch 27/250\n",
            "390/390 - 62s - loss: 1.2015 - accuracy: 0.7909 - val_loss: 1.2290 - val_accuracy: 0.7822\n",
            "Epoch 28/250\n",
            "390/390 - 62s - loss: 1.1962 - accuracy: 0.7949 - val_loss: 1.1368 - val_accuracy: 0.8086\n",
            "Epoch 29/250\n",
            "390/390 - 62s - loss: 1.2052 - accuracy: 0.7939 - val_loss: 1.1354 - val_accuracy: 0.8181\n",
            "Epoch 30/250\n",
            "390/390 - 62s - loss: 1.2063 - accuracy: 0.7964 - val_loss: 1.1194 - val_accuracy: 0.8241\n",
            "Epoch 31/250\n",
            "390/390 - 62s - loss: 1.2078 - accuracy: 0.7989 - val_loss: 1.2197 - val_accuracy: 0.7924\n",
            "Epoch 32/250\n",
            "390/390 - 63s - loss: 1.2147 - accuracy: 0.7981 - val_loss: 1.2105 - val_accuracy: 0.8039\n",
            "Epoch 33/250\n",
            "390/390 - 62s - loss: 1.2156 - accuracy: 0.8000 - val_loss: 1.1609 - val_accuracy: 0.8159\n",
            "Epoch 34/250\n",
            "390/390 - 62s - loss: 1.2188 - accuracy: 0.8002 - val_loss: 1.1953 - val_accuracy: 0.8135\n",
            "Epoch 35/250\n",
            "390/390 - 62s - loss: 1.2140 - accuracy: 0.8045 - val_loss: 1.1990 - val_accuracy: 0.8111\n",
            "Epoch 36/250\n",
            "390/390 - 62s - loss: 1.2294 - accuracy: 0.8021 - val_loss: 1.1447 - val_accuracy: 0.8262\n",
            "Epoch 37/250\n",
            "390/390 - 62s - loss: 1.2302 - accuracy: 0.8053 - val_loss: 1.1559 - val_accuracy: 0.8246\n",
            "Epoch 38/250\n",
            "390/390 - 62s - loss: 1.2358 - accuracy: 0.8034 - val_loss: 1.1793 - val_accuracy: 0.8227\n",
            "Epoch 39/250\n",
            "390/390 - 62s - loss: 1.2311 - accuracy: 0.8055 - val_loss: 1.1598 - val_accuracy: 0.8247\n",
            "Epoch 40/250\n",
            "390/390 - 62s - loss: 1.2275 - accuracy: 0.8066 - val_loss: 1.1657 - val_accuracy: 0.8248\n",
            "Epoch 41/250\n",
            "390/390 - 62s - loss: 1.1037 - accuracy: 0.8383 - val_loss: 0.9666 - val_accuracy: 0.8717\n",
            "Epoch 42/250\n",
            "390/390 - 62s - loss: 1.0316 - accuracy: 0.8451 - val_loss: 1.0293 - val_accuracy: 0.8388\n",
            "Epoch 43/250\n",
            "390/390 - 62s - loss: 1.0010 - accuracy: 0.8439 - val_loss: 0.9300 - val_accuracy: 0.8618\n",
            "Epoch 44/250\n",
            "390/390 - 62s - loss: 0.9803 - accuracy: 0.8449 - val_loss: 0.9362 - val_accuracy: 0.8564\n",
            "Epoch 45/250\n",
            "390/390 - 62s - loss: 0.9613 - accuracy: 0.8479 - val_loss: 0.9065 - val_accuracy: 0.8626\n",
            "Epoch 46/250\n",
            "390/390 - 62s - loss: 0.9544 - accuracy: 0.8492 - val_loss: 0.9152 - val_accuracy: 0.8607\n",
            "Epoch 47/250\n",
            "390/390 - 62s - loss: 0.9495 - accuracy: 0.8486 - val_loss: 0.9372 - val_accuracy: 0.8560\n",
            "Epoch 48/250\n",
            "390/390 - 62s - loss: 0.9595 - accuracy: 0.8461 - val_loss: 0.9115 - val_accuracy: 0.8625\n",
            "Epoch 49/250\n",
            "390/390 - 62s - loss: 0.9549 - accuracy: 0.8474 - val_loss: 0.8804 - val_accuracy: 0.8684\n",
            "Epoch 50/250\n",
            "390/390 - 62s - loss: 0.9579 - accuracy: 0.8460 - val_loss: 0.9400 - val_accuracy: 0.8485\n",
            "Epoch 51/250\n",
            "390/390 - 62s - loss: 0.9546 - accuracy: 0.8476 - val_loss: 0.9444 - val_accuracy: 0.8559\n",
            "Epoch 52/250\n",
            "390/390 - 62s - loss: 0.9571 - accuracy: 0.8480 - val_loss: 0.9510 - val_accuracy: 0.8516\n",
            "Epoch 53/250\n",
            "390/390 - 62s - loss: 0.9591 - accuracy: 0.8480 - val_loss: 0.8826 - val_accuracy: 0.8745\n",
            "Epoch 54/250\n",
            "390/390 - 62s - loss: 0.9560 - accuracy: 0.8512 - val_loss: 0.8964 - val_accuracy: 0.8692\n",
            "Epoch 55/250\n",
            "390/390 - 62s - loss: 0.9614 - accuracy: 0.8504 - val_loss: 0.8913 - val_accuracy: 0.8726\n",
            "Epoch 56/250\n",
            "390/390 - 62s - loss: 0.9557 - accuracy: 0.8536 - val_loss: 0.8839 - val_accuracy: 0.8764\n",
            "Epoch 57/250\n",
            "390/390 - 62s - loss: 0.9567 - accuracy: 0.8529 - val_loss: 0.9315 - val_accuracy: 0.8611\n",
            "Epoch 58/250\n",
            "390/390 - 62s - loss: 0.9648 - accuracy: 0.8501 - val_loss: 0.9324 - val_accuracy: 0.8647\n",
            "Epoch 59/250\n",
            "390/390 - 62s - loss: 0.9677 - accuracy: 0.8506 - val_loss: 1.0242 - val_accuracy: 0.8402\n",
            "Epoch 60/250\n",
            "390/390 - 62s - loss: 0.9670 - accuracy: 0.8543 - val_loss: 0.9091 - val_accuracy: 0.8712\n",
            "Epoch 61/250\n",
            "390/390 - 62s - loss: 0.8795 - accuracy: 0.8770 - val_loss: 0.8338 - val_accuracy: 0.8887\n",
            "Epoch 62/250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqZC3Sz_Z5--"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}