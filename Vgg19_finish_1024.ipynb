{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vgg19.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsM02dJFWwd2uGkRaWdE7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesliao2016/HCDR2018/blob/master/Vgg19_finish_1024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw2WjRAbYfm9",
        "outputId": "dc0b2cab-79e8-4562-bee3-624729657406"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def normalization(train_images, test_images):\n",
        "    mean = np.mean(train_images, axis=(0, 1, 2, 3))\n",
        "    std = np.std(train_images, axis=(0, 1, 2, 3))\n",
        "    train_images = (train_images - mean) / (std + 1e-7)\n",
        "    test_images = (test_images - mean) / (std + 1e-7)\n",
        "    return train_images, test_images\n",
        "\n",
        "\n",
        "def load_images():\n",
        "    (train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "    train_images = train_images.astype(np.float32)\n",
        "    test_images = test_images.astype(np.float32)\n",
        "\n",
        "    (train_images, test_images) = normalization(train_images, test_images)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 10)\n",
        "    test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "    # train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
        "    #     buffer_size=10000).batch(batch_size)\n",
        "    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "class ConvBNRelu(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel_size=3, strides=1, padding='SAME', weight_decay=0.0005, rate=0.4, drop=True):\n",
        "        super(ConvBNRelu, self).__init__()\n",
        "        self.drop = drop\n",
        "        self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
        "                                        padding=padding, kernel_regularizer=tf.keras.regularizers.l2(weight_decay))\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "        self.dropOut = keras.layers.Dropout(rate=rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        layer = self.conv(inputs)\n",
        "        layer = tf.nn.relu(layer)\n",
        "        layer = self.batchnorm(layer)\n",
        "        if self.drop:\n",
        "            layer = self.dropOut(layer)\n",
        "\n",
        "        return layer\n",
        "\n",
        "\n",
        "class VGG16Model(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(VGG16Model, self).__init__()\n",
        "        self.conv1 = ConvBNRelu(filters=64, kernel_size=[3, 3], rate=0.3)\n",
        "        self.conv2 = ConvBNRelu(filters=64, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling1 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv3 = ConvBNRelu(filters=128, kernel_size=[3, 3])\n",
        "        self.conv4 = ConvBNRelu(filters=128, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling2 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv5 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
        "        self.conv6 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
        "        self.conv7 = ConvBNRelu(filters=256, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling3 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv11 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv12 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv13 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling5 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.conv14 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv15 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
        "        self.conv16 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
        "        self.maxPooling6 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
        "        self.flat = keras.layers.Flatten()\n",
        "        self.dropOut = keras.layers.Dropout(rate=0.5)\n",
        "        self.dense1 = keras.layers.Dense(units=512,\n",
        "                                         activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = keras.layers.Dense(units=10)\n",
        "        self.softmax = keras.layers.Activation('softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        net = self.conv1(inputs)\n",
        "        net = self.conv2(net)\n",
        "        net = self.maxPooling1(net)\n",
        "        net = self.conv3(net)\n",
        "        net = self.conv4(net)\n",
        "        net = self.maxPooling2(net)\n",
        "        net = self.conv5(net)\n",
        "        net = self.conv6(net)\n",
        "        net = self.conv7(net)\n",
        "        net = self.maxPooling3(net)\n",
        "        net = self.conv11(net)\n",
        "        net = self.conv12(net)\n",
        "        net = self.conv13(net)\n",
        "        net = self.maxPooling5(net)\n",
        "        net = self.conv14(net)\n",
        "        net = self.conv15(net)\n",
        "        net = self.conv16(net)\n",
        "        net = self.maxPooling6(net)\n",
        "        net = self.dropOut(net)\n",
        "        net = self.flat(net)\n",
        "        net = self.dense1(net)\n",
        "        net = self.batchnorm(net)\n",
        "        net = self.dropOut(net)\n",
        "        net = self.dense2(net)\n",
        "        net = self.softmax(net)\n",
        "        return net\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(tf.__version__)\n",
        "    print(keras.__version__)\n",
        "\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    # if gpus:\n",
        "    #     # 텐서플로가 첫 번째 GPU만 사용하도록 제한\n",
        "    #     try:\n",
        "    #         print('start with GPU 7')\n",
        "    #         tf.config.experimental.set_visible_devices(gpus[7], 'GPU')\n",
        "    #         tf.config.experimental.set_memory_growth(gpus[7], True)\n",
        "    #     except RuntimeError as e:\n",
        "    #         # 프로그램 시작시에 접근 가능한 장치가 설정되어야만 합니다\n",
        "    #         print(e)\n",
        "\n",
        "    training_epochs = 250\n",
        "    batch_size = 128\n",
        "    learning_rate = 0.1\n",
        "    momentum = 0.9\n",
        "    lr_decay = 1e-6\n",
        "    lr_drop = 20\n",
        "\n",
        "    tf.random.set_seed(777)\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        return learning_rate * (0.5 ** (epoch // lr_drop))\n",
        "\n",
        "    reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_images()\n",
        "\n",
        "    # data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(train_images)\n",
        "\n",
        "    model = VGG16Model()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate,\n",
        "                                        decay=1e-6, momentum=momentum, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    model.fit_generator(datagen.flow(train_images, train_labels,\n",
        "                                     batch_size=batch_size), epochs=training_epochs, verbose=2, callbacks=[reduce_lr],\n",
        "                        steps_per_epoch=train_images.shape[0] // batch_size, validation_data=(test_images, test_labels))\n",
        "\n",
        "    model.save_weights('cifar10vgg_custom.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "2.6.0\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "390/390 - 67s - loss: 23.0235 - accuracy: 0.1904 - val_loss: 16.7154 - val_accuracy: 0.1250\n",
            "Epoch 2/250\n",
            "390/390 - 58s - loss: 11.6998 - accuracy: 0.2928 - val_loss: 9.1651 - val_accuracy: 0.1221\n",
            "Epoch 3/250\n",
            "390/390 - 58s - loss: 6.2258 - accuracy: 0.3986 - val_loss: 5.3452 - val_accuracy: 0.2229\n",
            "Epoch 4/250\n",
            "390/390 - 58s - loss: 3.6836 - accuracy: 0.4957 - val_loss: 3.3840 - val_accuracy: 0.3856\n",
            "Epoch 5/250\n",
            "390/390 - 58s - loss: 2.4862 - accuracy: 0.5583 - val_loss: 2.2444 - val_accuracy: 0.5493\n",
            "Epoch 6/250\n",
            "390/390 - 58s - loss: 1.9199 - accuracy: 0.6119 - val_loss: 1.8784 - val_accuracy: 0.5950\n",
            "Epoch 7/250\n",
            "390/390 - 58s - loss: 1.6991 - accuracy: 0.6374 - val_loss: 1.5457 - val_accuracy: 0.6783\n",
            "Epoch 8/250\n",
            "390/390 - 58s - loss: 1.5555 - accuracy: 0.6690 - val_loss: 1.4655 - val_accuracy: 0.6977\n",
            "Epoch 9/250\n",
            "390/390 - 58s - loss: 1.5099 - accuracy: 0.6826 - val_loss: 1.4904 - val_accuracy: 0.6745\n",
            "Epoch 10/250\n",
            "390/390 - 58s - loss: 1.4788 - accuracy: 0.6971 - val_loss: 1.5182 - val_accuracy: 0.6849\n",
            "Epoch 11/250\n",
            "390/390 - 58s - loss: 1.4578 - accuracy: 0.7080 - val_loss: 1.3814 - val_accuracy: 0.7338\n",
            "Epoch 12/250\n",
            "390/390 - 58s - loss: 1.4669 - accuracy: 0.7117 - val_loss: 1.3045 - val_accuracy: 0.7588\n",
            "Epoch 13/250\n",
            "390/390 - 58s - loss: 1.4611 - accuracy: 0.7212 - val_loss: 1.4242 - val_accuracy: 0.7354\n",
            "Epoch 14/250\n",
            "390/390 - 58s - loss: 1.4644 - accuracy: 0.7237 - val_loss: 1.4909 - val_accuracy: 0.7185\n",
            "Epoch 15/250\n",
            "390/390 - 58s - loss: 1.4750 - accuracy: 0.7269 - val_loss: 1.3632 - val_accuracy: 0.7654\n",
            "Epoch 16/250\n",
            "390/390 - 58s - loss: 1.4726 - accuracy: 0.7312 - val_loss: 1.3712 - val_accuracy: 0.7655\n",
            "Epoch 17/250\n",
            "390/390 - 58s - loss: 1.4780 - accuracy: 0.7353 - val_loss: 1.3623 - val_accuracy: 0.7727\n",
            "Epoch 18/250\n",
            "390/390 - 58s - loss: 1.4784 - accuracy: 0.7382 - val_loss: 1.4006 - val_accuracy: 0.7603\n",
            "Epoch 19/250\n",
            "390/390 - 58s - loss: 1.4998 - accuracy: 0.7353 - val_loss: 1.3999 - val_accuracy: 0.7672\n",
            "Epoch 20/250\n",
            "390/390 - 58s - loss: 1.5081 - accuracy: 0.7427 - val_loss: 1.5152 - val_accuracy: 0.7253\n",
            "Epoch 21/250\n",
            "390/390 - 58s - loss: 1.3196 - accuracy: 0.7833 - val_loss: 1.2409 - val_accuracy: 0.8030\n",
            "Epoch 22/250\n",
            "390/390 - 58s - loss: 1.2250 - accuracy: 0.7920 - val_loss: 1.1474 - val_accuracy: 0.8070\n",
            "Epoch 23/250\n",
            "390/390 - 58s - loss: 1.1982 - accuracy: 0.7921 - val_loss: 1.1077 - val_accuracy: 0.8186\n",
            "Epoch 24/250\n",
            "390/390 - 58s - loss: 1.1940 - accuracy: 0.7911 - val_loss: 1.1200 - val_accuracy: 0.8096\n",
            "Epoch 25/250\n",
            "390/390 - 58s - loss: 1.1941 - accuracy: 0.7920 - val_loss: 1.1285 - val_accuracy: 0.8138\n",
            "Epoch 26/250\n",
            "390/390 - 58s - loss: 1.1918 - accuracy: 0.7947 - val_loss: 1.1642 - val_accuracy: 0.8044\n",
            "Epoch 27/250\n",
            "390/390 - 58s - loss: 1.1963 - accuracy: 0.7953 - val_loss: 1.1249 - val_accuracy: 0.8133\n",
            "Epoch 28/250\n",
            "390/390 - 58s - loss: 1.1982 - accuracy: 0.7966 - val_loss: 1.1437 - val_accuracy: 0.8147\n",
            "Epoch 29/250\n",
            "390/390 - 58s - loss: 1.2106 - accuracy: 0.7967 - val_loss: 1.2152 - val_accuracy: 0.7963\n",
            "Epoch 30/250\n",
            "390/390 - 58s - loss: 1.1959 - accuracy: 0.8005 - val_loss: 1.2360 - val_accuracy: 0.8007\n",
            "Epoch 31/250\n",
            "390/390 - 58s - loss: 1.2074 - accuracy: 0.8005 - val_loss: 1.2343 - val_accuracy: 0.7974\n",
            "Epoch 32/250\n",
            "390/390 - 58s - loss: 1.2131 - accuracy: 0.7994 - val_loss: 1.1436 - val_accuracy: 0.8236\n",
            "Epoch 33/250\n",
            "390/390 - 58s - loss: 1.2056 - accuracy: 0.8041 - val_loss: 1.1326 - val_accuracy: 0.8241\n",
            "Epoch 34/250\n",
            "390/390 - 58s - loss: 1.2106 - accuracy: 0.8059 - val_loss: 1.1503 - val_accuracy: 0.8208\n",
            "Epoch 35/250\n",
            "390/390 - 58s - loss: 1.2172 - accuracy: 0.8027 - val_loss: 1.1008 - val_accuracy: 0.8424\n",
            "Epoch 36/250\n",
            "390/390 - 58s - loss: 1.2104 - accuracy: 0.8052 - val_loss: 1.1240 - val_accuracy: 0.8342\n",
            "Epoch 37/250\n",
            "390/390 - 58s - loss: 1.2135 - accuracy: 0.8060 - val_loss: 1.1523 - val_accuracy: 0.8302\n",
            "Epoch 38/250\n",
            "390/390 - 58s - loss: 1.2255 - accuracy: 0.8052 - val_loss: 1.1247 - val_accuracy: 0.8350\n",
            "Epoch 39/250\n",
            "390/390 - 58s - loss: 1.2208 - accuracy: 0.8082 - val_loss: 1.1941 - val_accuracy: 0.8102\n",
            "Epoch 40/250\n",
            "390/390 - 58s - loss: 1.2196 - accuracy: 0.8084 - val_loss: 1.1174 - val_accuracy: 0.8448\n",
            "Epoch 41/250\n",
            "390/390 - 58s - loss: 1.0963 - accuracy: 0.8410 - val_loss: 0.9854 - val_accuracy: 0.8680\n",
            "Epoch 42/250\n",
            "390/390 - 58s - loss: 1.0230 - accuracy: 0.8469 - val_loss: 0.9687 - val_accuracy: 0.8609\n",
            "Epoch 43/250\n",
            "390/390 - 58s - loss: 0.9812 - accuracy: 0.8505 - val_loss: 0.9554 - val_accuracy: 0.8533\n",
            "Epoch 44/250\n",
            "390/390 - 58s - loss: 0.9698 - accuracy: 0.8484 - val_loss: 0.8864 - val_accuracy: 0.8721\n",
            "Epoch 45/250\n",
            "390/390 - 58s - loss: 0.9575 - accuracy: 0.8480 - val_loss: 0.9315 - val_accuracy: 0.8568\n",
            "Epoch 46/250\n",
            "390/390 - 58s - loss: 0.9476 - accuracy: 0.8496 - val_loss: 0.9604 - val_accuracy: 0.8499\n",
            "Epoch 47/250\n",
            "390/390 - 58s - loss: 0.9443 - accuracy: 0.8493 - val_loss: 0.8689 - val_accuracy: 0.8771\n",
            "Epoch 48/250\n",
            "390/390 - 58s - loss: 0.9453 - accuracy: 0.8491 - val_loss: 1.0173 - val_accuracy: 0.8294\n",
            "Epoch 49/250\n",
            "390/390 - 58s - loss: 0.9492 - accuracy: 0.8487 - val_loss: 0.8845 - val_accuracy: 0.8686\n",
            "Epoch 50/250\n",
            "390/390 - 58s - loss: 0.9450 - accuracy: 0.8513 - val_loss: 0.9155 - val_accuracy: 0.8613\n",
            "Epoch 51/250\n",
            "390/390 - 58s - loss: 0.9474 - accuracy: 0.8504 - val_loss: 0.9178 - val_accuracy: 0.8644\n",
            "Epoch 52/250\n",
            "390/390 - 58s - loss: 0.9564 - accuracy: 0.8497 - val_loss: 0.9037 - val_accuracy: 0.8687\n",
            "Epoch 53/250\n",
            "390/390 - 58s - loss: 0.9566 - accuracy: 0.8498 - val_loss: 0.8829 - val_accuracy: 0.8695\n",
            "Epoch 54/250\n",
            "390/390 - 58s - loss: 0.9503 - accuracy: 0.8535 - val_loss: 0.9617 - val_accuracy: 0.8522\n",
            "Epoch 55/250\n",
            "390/390 - 58s - loss: 0.9498 - accuracy: 0.8543 - val_loss: 0.8759 - val_accuracy: 0.8782\n",
            "Epoch 56/250\n",
            "390/390 - 58s - loss: 0.9540 - accuracy: 0.8533 - val_loss: 0.9677 - val_accuracy: 0.8521\n",
            "Epoch 57/250\n",
            "390/390 - 58s - loss: 0.9582 - accuracy: 0.8527 - val_loss: 1.0110 - val_accuracy: 0.8407\n",
            "Epoch 58/250\n",
            "390/390 - 58s - loss: 0.9554 - accuracy: 0.8522 - val_loss: 0.9504 - val_accuracy: 0.8579\n",
            "Epoch 59/250\n",
            "390/390 - 58s - loss: 0.9615 - accuracy: 0.8532 - val_loss: 0.9067 - val_accuracy: 0.8723\n",
            "Epoch 60/250\n",
            "390/390 - 58s - loss: 0.9558 - accuracy: 0.8552 - val_loss: 0.9149 - val_accuracy: 0.8672\n",
            "Epoch 61/250\n",
            "390/390 - 58s - loss: 0.8745 - accuracy: 0.8786 - val_loss: 0.8098 - val_accuracy: 0.8933\n",
            "Epoch 62/250\n",
            "390/390 - 58s - loss: 0.8278 - accuracy: 0.8863 - val_loss: 0.8083 - val_accuracy: 0.8905\n",
            "Epoch 63/250\n",
            "390/390 - 58s - loss: 0.7976 - accuracy: 0.8902 - val_loss: 0.7738 - val_accuracy: 0.8953\n",
            "Epoch 64/250\n",
            "390/390 - 58s - loss: 0.7799 - accuracy: 0.8903 - val_loss: 0.7727 - val_accuracy: 0.8924\n",
            "Epoch 65/250\n",
            "390/390 - 58s - loss: 0.7662 - accuracy: 0.8911 - val_loss: 0.7345 - val_accuracy: 0.8973\n",
            "Epoch 66/250\n",
            "390/390 - 58s - loss: 0.7621 - accuracy: 0.8898 - val_loss: 0.8135 - val_accuracy: 0.8722\n",
            "Epoch 67/250\n",
            "390/390 - 58s - loss: 0.7537 - accuracy: 0.8884 - val_loss: 0.7314 - val_accuracy: 0.8932\n",
            "Epoch 68/250\n",
            "390/390 - 58s - loss: 0.7452 - accuracy: 0.8906 - val_loss: 0.7497 - val_accuracy: 0.8887\n",
            "Epoch 69/250\n",
            "390/390 - 58s - loss: 0.7412 - accuracy: 0.8904 - val_loss: 0.7034 - val_accuracy: 0.9012\n",
            "Epoch 70/250\n",
            "390/390 - 58s - loss: 0.7356 - accuracy: 0.8905 - val_loss: 0.7265 - val_accuracy: 0.8945\n",
            "Epoch 71/250\n",
            "390/390 - 58s - loss: 0.7401 - accuracy: 0.8898 - val_loss: 0.7531 - val_accuracy: 0.8841\n",
            "Epoch 72/250\n",
            "390/390 - 58s - loss: 0.7358 - accuracy: 0.8902 - val_loss: 0.7353 - val_accuracy: 0.8901\n",
            "Epoch 73/250\n",
            "390/390 - 58s - loss: 0.7389 - accuracy: 0.8888 - val_loss: 0.7227 - val_accuracy: 0.8974\n",
            "Epoch 74/250\n",
            "390/390 - 58s - loss: 0.7343 - accuracy: 0.8905 - val_loss: 0.7380 - val_accuracy: 0.8921\n",
            "Epoch 75/250\n",
            "390/390 - 58s - loss: 0.7347 - accuracy: 0.8901 - val_loss: 0.7480 - val_accuracy: 0.8859\n",
            "Epoch 76/250\n",
            "390/390 - 58s - loss: 0.7317 - accuracy: 0.8882 - val_loss: 0.7633 - val_accuracy: 0.8883\n",
            "Epoch 77/250\n",
            "390/390 - 58s - loss: 0.7383 - accuracy: 0.8883 - val_loss: 0.7369 - val_accuracy: 0.8932\n",
            "Epoch 78/250\n",
            "390/390 - 58s - loss: 0.7276 - accuracy: 0.8924 - val_loss: 0.7508 - val_accuracy: 0.8891\n",
            "Epoch 79/250\n",
            "390/390 - 58s - loss: 0.7348 - accuracy: 0.8909 - val_loss: 0.7309 - val_accuracy: 0.8934\n",
            "Epoch 80/250\n",
            "390/390 - 58s - loss: 0.7309 - accuracy: 0.8923 - val_loss: 0.7355 - val_accuracy: 0.8869\n",
            "Epoch 81/250\n",
            "390/390 - 58s - loss: 0.6819 - accuracy: 0.9059 - val_loss: 0.7039 - val_accuracy: 0.9016\n",
            "Epoch 82/250\n",
            "390/390 - 58s - loss: 0.6416 - accuracy: 0.9159 - val_loss: 0.6755 - val_accuracy: 0.9090\n",
            "Epoch 83/250\n",
            "390/390 - 58s - loss: 0.6270 - accuracy: 0.9193 - val_loss: 0.6501 - val_accuracy: 0.9101\n",
            "Epoch 84/250\n",
            "390/390 - 58s - loss: 0.6155 - accuracy: 0.9183 - val_loss: 0.6634 - val_accuracy: 0.9068\n",
            "Epoch 85/250\n",
            "390/390 - 58s - loss: 0.6031 - accuracy: 0.9217 - val_loss: 0.6537 - val_accuracy: 0.9096\n",
            "Epoch 86/250\n",
            "390/390 - 58s - loss: 0.5979 - accuracy: 0.9207 - val_loss: 0.6288 - val_accuracy: 0.9131\n",
            "Epoch 87/250\n",
            "390/390 - 58s - loss: 0.5901 - accuracy: 0.9217 - val_loss: 0.6365 - val_accuracy: 0.9077\n",
            "Epoch 88/250\n",
            "390/390 - 58s - loss: 0.5881 - accuracy: 0.9207 - val_loss: 0.6212 - val_accuracy: 0.9136\n",
            "Epoch 89/250\n",
            "390/390 - 58s - loss: 0.5782 - accuracy: 0.9214 - val_loss: 0.6328 - val_accuracy: 0.9104\n",
            "Epoch 90/250\n",
            "390/390 - 58s - loss: 0.5750 - accuracy: 0.9223 - val_loss: 0.6185 - val_accuracy: 0.9093\n",
            "Epoch 91/250\n",
            "390/390 - 58s - loss: 0.5725 - accuracy: 0.9213 - val_loss: 0.6428 - val_accuracy: 0.9042\n",
            "Epoch 92/250\n",
            "390/390 - 58s - loss: 0.5661 - accuracy: 0.9227 - val_loss: 0.6069 - val_accuracy: 0.9108\n",
            "Epoch 93/250\n",
            "390/390 - 58s - loss: 0.5625 - accuracy: 0.9232 - val_loss: 0.6183 - val_accuracy: 0.9113\n",
            "Epoch 94/250\n",
            "390/390 - 58s - loss: 0.5672 - accuracy: 0.9212 - val_loss: 0.6093 - val_accuracy: 0.9105\n",
            "Epoch 95/250\n",
            "390/390 - 58s - loss: 0.5586 - accuracy: 0.9227 - val_loss: 0.6171 - val_accuracy: 0.9051\n",
            "Epoch 96/250\n",
            "390/390 - 58s - loss: 0.5544 - accuracy: 0.9223 - val_loss: 0.6319 - val_accuracy: 0.9037\n",
            "Epoch 97/250\n",
            "390/390 - 58s - loss: 0.5552 - accuracy: 0.9235 - val_loss: 0.6125 - val_accuracy: 0.9076\n",
            "Epoch 98/250\n",
            "390/390 - 58s - loss: 0.5602 - accuracy: 0.9216 - val_loss: 0.6001 - val_accuracy: 0.9094\n",
            "Epoch 99/250\n",
            "390/390 - 58s - loss: 0.5543 - accuracy: 0.9222 - val_loss: 0.6339 - val_accuracy: 0.9028\n",
            "Epoch 100/250\n",
            "390/390 - 58s - loss: 0.5526 - accuracy: 0.9225 - val_loss: 0.6118 - val_accuracy: 0.9096\n",
            "Epoch 101/250\n",
            "390/390 - 58s - loss: 0.5171 - accuracy: 0.9345 - val_loss: 0.5604 - val_accuracy: 0.9234\n",
            "Epoch 102/250\n",
            "390/390 - 58s - loss: 0.4971 - accuracy: 0.9391 - val_loss: 0.5675 - val_accuracy: 0.9200\n",
            "Epoch 103/250\n",
            "390/390 - 58s - loss: 0.4884 - accuracy: 0.9411 - val_loss: 0.5659 - val_accuracy: 0.9197\n",
            "Epoch 104/250\n",
            "390/390 - 58s - loss: 0.4793 - accuracy: 0.9425 - val_loss: 0.5500 - val_accuracy: 0.9240\n",
            "Epoch 105/250\n",
            "390/390 - 58s - loss: 0.4698 - accuracy: 0.9430 - val_loss: 0.5467 - val_accuracy: 0.9235\n",
            "Epoch 106/250\n",
            "390/390 - 58s - loss: 0.4628 - accuracy: 0.9453 - val_loss: 0.5459 - val_accuracy: 0.9242\n",
            "Epoch 107/250\n",
            "390/390 - 58s - loss: 0.4587 - accuracy: 0.9462 - val_loss: 0.5390 - val_accuracy: 0.9253\n",
            "Epoch 108/250\n",
            "390/390 - 58s - loss: 0.4563 - accuracy: 0.9459 - val_loss: 0.5490 - val_accuracy: 0.9227\n",
            "Epoch 109/250\n",
            "390/390 - 58s - loss: 0.4563 - accuracy: 0.9446 - val_loss: 0.5748 - val_accuracy: 0.9171\n",
            "Epoch 110/250\n",
            "390/390 - 58s - loss: 0.4486 - accuracy: 0.9461 - val_loss: 0.5414 - val_accuracy: 0.9218\n",
            "Epoch 111/250\n",
            "390/390 - 58s - loss: 0.4468 - accuracy: 0.9466 - val_loss: 0.5482 - val_accuracy: 0.9213\n",
            "Epoch 112/250\n",
            "390/390 - 58s - loss: 0.4410 - accuracy: 0.9466 - val_loss: 0.5256 - val_accuracy: 0.9248\n",
            "Epoch 113/250\n",
            "390/390 - 58s - loss: 0.4390 - accuracy: 0.9470 - val_loss: 0.5435 - val_accuracy: 0.9202\n",
            "Epoch 114/250\n",
            "390/390 - 58s - loss: 0.4364 - accuracy: 0.9459 - val_loss: 0.5465 - val_accuracy: 0.9179\n",
            "Epoch 115/250\n",
            "390/390 - 58s - loss: 0.4338 - accuracy: 0.9474 - val_loss: 0.5328 - val_accuracy: 0.9249\n",
            "Epoch 116/250\n",
            "390/390 - 58s - loss: 0.4330 - accuracy: 0.9471 - val_loss: 0.5281 - val_accuracy: 0.9223\n",
            "Epoch 117/250\n",
            "390/390 - 58s - loss: 0.4300 - accuracy: 0.9460 - val_loss: 0.5358 - val_accuracy: 0.9223\n",
            "Epoch 118/250\n",
            "390/390 - 58s - loss: 0.4299 - accuracy: 0.9461 - val_loss: 0.5434 - val_accuracy: 0.9183\n",
            "Epoch 119/250\n",
            "390/390 - 58s - loss: 0.4245 - accuracy: 0.9476 - val_loss: 0.5426 - val_accuracy: 0.9200\n",
            "Epoch 120/250\n",
            "390/390 - 58s - loss: 0.4274 - accuracy: 0.9446 - val_loss: 0.5204 - val_accuracy: 0.9231\n",
            "Epoch 121/250\n",
            "390/390 - 58s - loss: 0.4044 - accuracy: 0.9536 - val_loss: 0.5123 - val_accuracy: 0.9296\n",
            "Epoch 122/250\n",
            "390/390 - 58s - loss: 0.3910 - accuracy: 0.9569 - val_loss: 0.5211 - val_accuracy: 0.9269\n",
            "Epoch 123/250\n",
            "390/390 - 58s - loss: 0.3886 - accuracy: 0.9580 - val_loss: 0.5144 - val_accuracy: 0.9278\n",
            "Epoch 124/250\n",
            "390/390 - 58s - loss: 0.3786 - accuracy: 0.9601 - val_loss: 0.5042 - val_accuracy: 0.9290\n",
            "Epoch 125/250\n",
            "390/390 - 58s - loss: 0.3759 - accuracy: 0.9613 - val_loss: 0.5123 - val_accuracy: 0.9254\n",
            "Epoch 126/250\n",
            "390/390 - 58s - loss: 0.3761 - accuracy: 0.9596 - val_loss: 0.5082 - val_accuracy: 0.9276\n",
            "Epoch 127/250\n",
            "390/390 - 58s - loss: 0.3670 - accuracy: 0.9617 - val_loss: 0.5162 - val_accuracy: 0.9270\n",
            "Epoch 128/250\n",
            "390/390 - 58s - loss: 0.3683 - accuracy: 0.9613 - val_loss: 0.5017 - val_accuracy: 0.9294\n",
            "Epoch 129/250\n",
            "390/390 - 58s - loss: 0.3584 - accuracy: 0.9647 - val_loss: 0.5119 - val_accuracy: 0.9284\n",
            "Epoch 130/250\n",
            "390/390 - 58s - loss: 0.3610 - accuracy: 0.9623 - val_loss: 0.5045 - val_accuracy: 0.9296\n",
            "Epoch 131/250\n",
            "390/390 - 58s - loss: 0.3570 - accuracy: 0.9626 - val_loss: 0.5116 - val_accuracy: 0.9269\n",
            "Epoch 132/250\n",
            "390/390 - 58s - loss: 0.3577 - accuracy: 0.9616 - val_loss: 0.5057 - val_accuracy: 0.9279\n",
            "Epoch 133/250\n",
            "390/390 - 58s - loss: 0.3524 - accuracy: 0.9636 - val_loss: 0.4943 - val_accuracy: 0.9297\n",
            "Epoch 134/250\n",
            "390/390 - 58s - loss: 0.3526 - accuracy: 0.9634 - val_loss: 0.4911 - val_accuracy: 0.9303\n",
            "Epoch 135/250\n",
            "390/390 - 58s - loss: 0.3470 - accuracy: 0.9632 - val_loss: 0.5013 - val_accuracy: 0.9296\n",
            "Epoch 136/250\n",
            "390/390 - 58s - loss: 0.3474 - accuracy: 0.9641 - val_loss: 0.4916 - val_accuracy: 0.9300\n",
            "Epoch 137/250\n",
            "390/390 - 58s - loss: 0.3456 - accuracy: 0.9634 - val_loss: 0.4872 - val_accuracy: 0.9298\n",
            "Epoch 138/250\n",
            "390/390 - 58s - loss: 0.3448 - accuracy: 0.9632 - val_loss: 0.5000 - val_accuracy: 0.9283\n",
            "Epoch 139/250\n",
            "390/390 - 58s - loss: 0.3414 - accuracy: 0.9649 - val_loss: 0.4988 - val_accuracy: 0.9260\n",
            "Epoch 140/250\n",
            "390/390 - 58s - loss: 0.3380 - accuracy: 0.9649 - val_loss: 0.4972 - val_accuracy: 0.9264\n",
            "Epoch 141/250\n",
            "390/390 - 58s - loss: 0.3319 - accuracy: 0.9664 - val_loss: 0.4823 - val_accuracy: 0.9309\n",
            "Epoch 142/250\n",
            "390/390 - 58s - loss: 0.3227 - accuracy: 0.9696 - val_loss: 0.4735 - val_accuracy: 0.9333\n",
            "Epoch 143/250\n",
            "390/390 - 57s - loss: 0.3188 - accuracy: 0.9704 - val_loss: 0.4780 - val_accuracy: 0.9337\n",
            "Epoch 144/250\n",
            "390/390 - 58s - loss: 0.3135 - accuracy: 0.9721 - val_loss: 0.4779 - val_accuracy: 0.9332\n",
            "Epoch 145/250\n",
            "390/390 - 58s - loss: 0.3144 - accuracy: 0.9720 - val_loss: 0.4774 - val_accuracy: 0.9328\n",
            "Epoch 146/250\n",
            "390/390 - 58s - loss: 0.3117 - accuracy: 0.9716 - val_loss: 0.4755 - val_accuracy: 0.9334\n",
            "Epoch 147/250\n",
            "390/390 - 58s - loss: 0.3083 - accuracy: 0.9736 - val_loss: 0.4792 - val_accuracy: 0.9312\n",
            "Epoch 148/250\n",
            "390/390 - 58s - loss: 0.3090 - accuracy: 0.9716 - val_loss: 0.4811 - val_accuracy: 0.9327\n",
            "Epoch 149/250\n",
            "390/390 - 58s - loss: 0.3039 - accuracy: 0.9737 - val_loss: 0.4705 - val_accuracy: 0.9339\n",
            "Epoch 150/250\n",
            "390/390 - 58s - loss: 0.3030 - accuracy: 0.9730 - val_loss: 0.4734 - val_accuracy: 0.9335\n",
            "Epoch 151/250\n",
            "390/390 - 58s - loss: 0.3009 - accuracy: 0.9733 - val_loss: 0.4756 - val_accuracy: 0.9328\n",
            "Epoch 152/250\n",
            "390/390 - 58s - loss: 0.3002 - accuracy: 0.9738 - val_loss: 0.4720 - val_accuracy: 0.9348\n",
            "Epoch 153/250\n",
            "390/390 - 58s - loss: 0.3026 - accuracy: 0.9728 - val_loss: 0.4710 - val_accuracy: 0.9352\n",
            "Epoch 154/250\n",
            "390/390 - 58s - loss: 0.3011 - accuracy: 0.9730 - val_loss: 0.4810 - val_accuracy: 0.9338\n",
            "Epoch 155/250\n",
            "390/390 - 58s - loss: 0.2958 - accuracy: 0.9747 - val_loss: 0.4746 - val_accuracy: 0.9349\n",
            "Epoch 156/250\n",
            "390/390 - 58s - loss: 0.2942 - accuracy: 0.9748 - val_loss: 0.4697 - val_accuracy: 0.9328\n",
            "Epoch 157/250\n",
            "390/390 - 58s - loss: 0.2973 - accuracy: 0.9736 - val_loss: 0.4732 - val_accuracy: 0.9334\n",
            "Epoch 158/250\n",
            "390/390 - 58s - loss: 0.2915 - accuracy: 0.9744 - val_loss: 0.4741 - val_accuracy: 0.9309\n",
            "Epoch 159/250\n",
            "390/390 - 58s - loss: 0.2897 - accuracy: 0.9749 - val_loss: 0.4782 - val_accuracy: 0.9314\n",
            "Epoch 160/250\n",
            "390/390 - 58s - loss: 0.2884 - accuracy: 0.9753 - val_loss: 0.4830 - val_accuracy: 0.9303\n",
            "Epoch 161/250\n",
            "390/390 - 58s - loss: 0.2824 - accuracy: 0.9770 - val_loss: 0.4660 - val_accuracy: 0.9355\n",
            "Epoch 162/250\n",
            "390/390 - 58s - loss: 0.2841 - accuracy: 0.9768 - val_loss: 0.4661 - val_accuracy: 0.9347\n",
            "Epoch 163/250\n",
            "390/390 - 58s - loss: 0.2792 - accuracy: 0.9776 - val_loss: 0.4690 - val_accuracy: 0.9325\n",
            "Epoch 164/250\n",
            "390/390 - 58s - loss: 0.2777 - accuracy: 0.9782 - val_loss: 0.4696 - val_accuracy: 0.9345\n",
            "Epoch 165/250\n",
            "390/390 - 58s - loss: 0.2767 - accuracy: 0.9784 - val_loss: 0.4670 - val_accuracy: 0.9360\n",
            "Epoch 166/250\n",
            "390/390 - 58s - loss: 0.2744 - accuracy: 0.9789 - val_loss: 0.4593 - val_accuracy: 0.9367\n",
            "Epoch 167/250\n",
            "390/390 - 58s - loss: 0.2782 - accuracy: 0.9778 - val_loss: 0.4676 - val_accuracy: 0.9348\n",
            "Epoch 168/250\n",
            "390/390 - 58s - loss: 0.2762 - accuracy: 0.9773 - val_loss: 0.4706 - val_accuracy: 0.9329\n",
            "Epoch 169/250\n",
            "390/390 - 58s - loss: 0.2743 - accuracy: 0.9792 - val_loss: 0.4654 - val_accuracy: 0.9343\n",
            "Epoch 170/250\n",
            "390/390 - 58s - loss: 0.2716 - accuracy: 0.9786 - val_loss: 0.4642 - val_accuracy: 0.9343\n",
            "Epoch 171/250\n",
            "390/390 - 58s - loss: 0.2686 - accuracy: 0.9795 - val_loss: 0.4699 - val_accuracy: 0.9331\n",
            "Epoch 172/250\n",
            "390/390 - 58s - loss: 0.2724 - accuracy: 0.9780 - val_loss: 0.4757 - val_accuracy: 0.9327\n",
            "Epoch 173/250\n",
            "390/390 - 58s - loss: 0.2685 - accuracy: 0.9794 - val_loss: 0.4634 - val_accuracy: 0.9356\n",
            "Epoch 174/250\n",
            "390/390 - 58s - loss: 0.2696 - accuracy: 0.9791 - val_loss: 0.4634 - val_accuracy: 0.9362\n",
            "Epoch 175/250\n",
            "390/390 - 58s - loss: 0.2685 - accuracy: 0.9798 - val_loss: 0.4731 - val_accuracy: 0.9323\n",
            "Epoch 176/250\n",
            "390/390 - 58s - loss: 0.2672 - accuracy: 0.9805 - val_loss: 0.4650 - val_accuracy: 0.9341\n",
            "Epoch 177/250\n",
            "390/390 - 58s - loss: 0.2656 - accuracy: 0.9796 - val_loss: 0.4656 - val_accuracy: 0.9353\n",
            "Epoch 178/250\n",
            "390/390 - 58s - loss: 0.2662 - accuracy: 0.9796 - val_loss: 0.4778 - val_accuracy: 0.9330\n",
            "Epoch 179/250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqZC3Sz_Z5--"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}